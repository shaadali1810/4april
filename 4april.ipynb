{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9b0abe-a971-49a0-b97d-b206b275cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Decision trees is a type of supervised machine learning algorithm that is used by the Train Using AutoML tool and classifies or regresses the data using true or false answers to certain questions. The resulting structure, when visualized, is in the form of a tree with different types of nodes—root, internal, and leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4281f859-85b2-4a53-b805-bd96fcf9799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 A decision tree is one of the popular and powerful machine learning algorithms that I have learned. It is a non-parametric supervised learning method that can be used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. For a classification model, the target values are discrete in nature, whereas, for a regression model, the target values are represented by continuous values. Unlike the black box type of algorithms such as Neural Network, Decision Trees are comparably easier to understand because it shares internal decision-making logic (you will find details in the following session).\n",
    "\n",
    "#Despite the fact that many data scientists believe it’s an old method and they may have some doubts of its accuracy due to an overfitting problem, the more recent tree-based models, for example, Random forest (bagging method), gradient boosting (boosting method) and XGBoost (boosting method) are built on the top of decision tree algorithm. Therefore, the concepts and algorithms behind Decision Trees are strongly worth understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcacb57-e497-4390-be9d-d7a121748254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3fter the first split, the decision tree algorithm examines each of the two subsets of data and finds a predictor variable and a value that gives the most information. The process continues until a program-specified maximum tree depth is reached. There are several algorithms to split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9abaf9-385b-4b3a-a125-afa57c7a6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Every decision that we make at internal node, it will create a hyperplane – in other words, corresponding to every decision / splitting of the data set, we will have a hyperplane. All of your hyperplanes are axis-parallel – they are parallel to either X-axis or Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493774b0-e705-40de-96a0-11a8329e07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b21ca6-3776-4e54-b99b-cc48f0b18ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 An example of the confusion matrix we may obtain with the trained model is shown above for this example dataset. This gives us a lot more information than just the accuracy of the model.\n",
    "\n",
    "#Adding the numbers in the first column, we see that the total samples in the positive class are 45+15=60. Similarly, adding the numbers in the second column gives us the number of samples in the negative class, which is 40 in this case. The sum of the numbers in all the boxes gives the total number of samples evaluated. Further, the correct classifications are the diagonal elements of the matrix—45 for the positive class and 32 for the negative class.\n",
    "\n",
    "#Now, 15 samples (bottom-left box) that were expected to be of the positive class were classified as the negative class by the model. So it is called “False Negatives” because the model predicted “negative,” which was wrong. Similarly, 8 samples (top-right box) were expected to be of negative class but were classified as “positive” by the model. They are thus called “False Positives.” We can evaluate the model more closely using these four different numbers from the matrix.\n",
    "\n",
    "\n",
    "#Precision (for the positive class). The number of samples actually belonging to the positive class out of all the samples that were predicted to be of the positive class by the model.\n",
    "#Recall (for the positive class). The number of samples predicted correctly to be belonging to the positive class out of all the samples that actually belong to the positive class\n",
    "#F1-Score (for the positive class). The harmonic mean of the precision and recall scores obtained for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d36895b-5b38-4b69-96fd-4cf25ce5bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 A classification problem is characterized by the prediction of the category or class of a given observation based on its corresponding features. The choice of the most appropriate evaluation metric will depend on the aspects of model performance the user would like to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603e2259-cf41-440d-9a4b-b5135ea82072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 As a result, you want to minimize false positive errors. In this case, precision is a good metric to evaluate and optimize for. A higher precision score indicates that the model makes fewer false positive predictions. It is more likely to be correct whenever it predicts a positive outcome.\n",
    "#In most high-risk disease detection cases (like cancer), recall is a more important evaluation metric than precision. However, precision is more useful when we want to affirm the correctness of our model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bed02-fad5-4dbf-822b-78dfa5f24686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
